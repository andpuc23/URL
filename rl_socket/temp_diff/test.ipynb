{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snipercapt\\.conda\\envs\\Reinforsement_learning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.linalg as LA\n",
    "\n",
    "from camera_transition import CameraTransition\n",
    "from models import (\n",
    "    ActorImprovedValue,\n",
    "    CriticTD,\n",
    "    ActorModel,\n",
    "    CriticModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_rodrigues(mat):\n",
    "    U, _, V_T = LA.svd(mat)\n",
    "    R = U @ V_T\n",
    "    r1 = R[:, 2, 1] - R[:, 1, 2]\n",
    "    r2 = R[:, 0, 2] - R[:, 2, 0]\n",
    "    r3 = R[:, 1, 0] - R[:, 0, 1]\n",
    "\n",
    "    r = torch.stack((r1, r2, r3), 1)\n",
    "    s = LA.norm(r, dim=1) / 2\n",
    "    c = (R[:, 0, 0] + R[:, 1, 1] + R[:, 2, 2] - 1) / 2\n",
    "    c = torch.clip(c, -1., 1.)\n",
    "    theta = torch.acos(c)\n",
    "    vth = 1 / (2 * s)\n",
    "    r = r.T * vth * theta\n",
    "    return r.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "camera_mat = torch.tensor([\n",
    "    [633.09029639, 0., 629.06462963],\n",
    "    [0., 638.7544391, 362.74013262],\n",
    "    [0., 0., 1.]\n",
    "])\n",
    "target_points = torch.tensor([\n",
    "    [822.75,     362.25],\n",
    "    [755.25,     476.25],\n",
    "    [624.75,     476.25],\n",
    "    [554.25,     366.75],\n",
    "    [618.75,     273.75],\n",
    "    [755.25,     270.75]\n",
    "    ])\n",
    "points_env = torch.tensor([\n",
    "    [2, 0, 4],\n",
    "    [1, 1.66, 4],\n",
    "    [-1, 1.66, 4],\n",
    "    [-2, 0, 4],\n",
    "    [-1, -1.66, 4],\n",
    "    [1, -1.66, 4],\n",
    "])\n",
    "\n",
    "camera_transition = CameraTransition(\n",
    "    device,\n",
    "    camera_mat,\n",
    "    target_points,\n",
    "    points_env,\n",
    "    100.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_critic_kind = torch.optim.Adam\n",
    "optimizer_critic_parameters = {\n",
    "    'lr': 1e-5,\n",
    "    'weight_decay': 1e-5\n",
    "}\n",
    "\n",
    "optimizer_actor_kind = torch.optim.Adam\n",
    "optimizer_actor_parameters = {\n",
    "    \"lr\" : 1e-5,\n",
    "    \"weight_decay\" : 1e-5\n",
    "}\n",
    "\n",
    "critic_iterations = 300\n",
    "critic_batch_size = 200\n",
    "\n",
    "actor_iterations = 300\n",
    "actor_batch_size = 200\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = ActorModel().to(device)\n",
    "critic = CriticModel().to(device)\n",
    "\n",
    "critic_temporal_difference = CriticTD(actor, critic, camera_transition).to(device)\n",
    "actor_improved_value = ActorImprovedValue(actor, critic, camera_transition).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_position():\n",
    "    return torch.Tensor([*np.random.uniform(-.1, .1, 3).astype(np.float64), *np.random.uniform(-.5, .5, 3).astype(np.float64)])\n",
    "\n",
    "SCALES = torch.tensor([0.1, 0.1, 0.1, 0.5, 0.5, 0.5])\n",
    "def critic_epoch(optimizer: torch.optim.Optimizer,\n",
    "                 model: CriticTD, \n",
    "                 iterations: int, \n",
    "                 batch_size: int) -> List[float]:\n",
    "    losses = []\n",
    "    for iteration in tqdm(range(iterations), \"Critic epoch\"):\n",
    "        # Поменять на наши ограничения позиции\n",
    "#         scales = SCALES\n",
    "#         X = (torch.rand((batch_size, 6)) * scales * 2 - scales).to(device)\n",
    "        X = get_start_position().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "    print(\"Critic mean loss:\", np.mean(losses), \"[%f --> %f]\" % (np.mean(losses[0:10]), np.mean(losses[-10:-1])))\n",
    "    return losses\n",
    "\n",
    "def actor_epoch(optimizer: torch.optim.Optimizer,\n",
    "                 model: CriticTD, \n",
    "                 iterations: int, \n",
    "                 batch_size: int) -> List[float]:\n",
    "    values = []\n",
    "    for iteration in tqdm(range(iterations), \"Actor epoch\"):\n",
    "        # Поменять на наши ограничения позиции\n",
    "#         scales = SCALES\n",
    "#         X = (torch.rand((batch_size, 6)) * scales * 2 - scales).to(device)\n",
    "        X = get_start_position().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        improved_value = model(X)\n",
    "        improved_value.backward()\n",
    "        optimizer.step()\n",
    "        values.append(improved_value.detach().cpu().numpy())\n",
    "    print(\"Actor mean value:\", np.mean(values), \"[%f --> %f]\" % (np.mean(values[0:10]), np.mean(values[-10:-1])))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_actor = optimizer_actor_kind(actor_improved_value.parameters(), **optimizer_actor_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actor-Critic learning:   0%|          | 0/100 [00:00<?, ?it/s]\n",
      "Critic epoch:   0%|          | 0/300 [00:00<?, ?it/s]\u001b[A\n",
      "Actor-Critic learning:   0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActor-Critic learning\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     optimizer_critic \u001b[38;5;241m=\u001b[39m optimizer_critic_kind(critic_temporal_difference\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptimizer_critic_parameters) \u001b[38;5;66;03m## It is important to reinitialize the critic optimizer to erase irrelevant momenta and adaptations\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mcritic_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_critic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcritic_temporal_difference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcritic_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mactor_batch_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      7\u001b[0m     actor_epoch(optimizer_actor, \n\u001b[0;32m      8\u001b[0m                 actor_improved_value,\n\u001b[0;32m      9\u001b[0m                 actor_iterations,\n\u001b[0;32m     10\u001b[0m                 actor_batch_size)\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mcritic_epoch\u001b[1;34m(optimizer, model, iterations, batch_size)\u001b[0m\n\u001b[0;32m     14\u001b[0m X \u001b[38;5;241m=\u001b[39m get_start_position()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\.conda\\envs\\Reinforsement_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\RL\\rl_sk\\FP\\URL\\rl_socket\\temp_diff\\models.py:126\u001b[0m, in \u001b[0;36mCriticTD.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    125\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[1;32m--> 126\u001b[0m     next_state, reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m# print(self.critic(next_state))\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     td_target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msatellite_discount \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(next_state)\n",
      "File \u001b[1;32m~\\.conda\\envs\\Reinforsement_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\RL\\rl_sk\\FP\\URL\\rl_socket\\temp_diff\\camera_transition.py:109\u001b[0m, in \u001b[0;36mCameraTransition.forward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    105\u001b[0m             state: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    106\u001b[0m             action: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    108\u001b[0m     state \u001b[38;5;241m=\u001b[39m state \u001b[38;5;241m+\u001b[39m action\n\u001b[1;32m--> 109\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_reward(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_project_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state, \u001b[38;5;241m-\u001b[39mreward\n",
      "File \u001b[1;32m~\\Desktop\\RL\\rl_sk\\FP\\URL\\rl_socket\\temp_diff\\camera_transition.py:45\u001b[0m, in \u001b[0;36mCameraTransition._project_points\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_project_points\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     42\u001b[0m                     state: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     44\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 45\u001b[0m     tvec \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     46\u001b[0m     angles \u001b[38;5;241m=\u001b[39m state[:, \u001b[38;5;241m3\u001b[39m:]\n\u001b[0;32m     48\u001b[0m     rmat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_rmat(angles)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(epochs), \"Actor-Critic learning\"):\n",
    "    optimizer_critic = optimizer_critic_kind(critic_temporal_difference.parameters(), **optimizer_critic_parameters) ## It is important to reinitialize the critic optimizer to erase irrelevant momenta and adaptations\n",
    "    values = np.array(critic_epoch(optimizer_critic, \n",
    "                                   critic_temporal_difference,\n",
    "                                   critic_iterations,\n",
    "                                   actor_batch_size))\n",
    "    actor_epoch(optimizer_actor, \n",
    "                actor_improved_value,\n",
    "                actor_iterations,\n",
    "                actor_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor(torch.tensor([0., 0., 1., 0.1, 0.1, 0.]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter( np.array([\n",
    "    [822.75,     362.25],\n",
    "    [755.25,     476.25],\n",
    "    [624.75,     476.25],\n",
    "    [554.25,     366.75],\n",
    "    [618.75,     273.75],\n",
    "    [755.25,     270.75]])[:,0],\n",
    "            np.array([\n",
    "                [822.75,     362.25],\n",
    "    [755.25,     476.25],\n",
    "    [624.75,     476.25],\n",
    "    [554.25,     366.75],\n",
    "    [618.75,     273.75],\n",
    "    [755.25,     270.75]])[:,1], c=range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0415, -0.0568, -0.0732,  0.3749,  0.2339,  0.3368])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([*np.random.uniform(-.1, .1, 3).astype(np.float64), *np.random.uniform(-.5, .5, 3).astype(np.float64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "45e6302f2b4ec81add0d3758726e649dffe43b2cfc8a72d111ea985a61503901"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
